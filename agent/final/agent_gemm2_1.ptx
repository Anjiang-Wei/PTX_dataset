//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	_Z15sgemm_optimizedPKfS0_Pf
// _ZZ15sgemm_optimizedPKfS0_PfE2As has been demoted
// _ZZ15sgemm_optimizedPKfS0_PfE2Bs has been demoted
// _ZZ15sgemm_optimizedPKfS0_PfE6C_smem has been demoted

.visible .entry _Z15sgemm_optimizedPKfS0_Pf(
	.param .u64 _Z15sgemm_optimizedPKfS0_Pf_param_0,
	.param .u64 _Z15sgemm_optimizedPKfS0_Pf_param_1,
	.param .u64 _Z15sgemm_optimizedPKfS0_Pf_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<44>;
	.reg .b32 	%r<114>;
	.reg .b64 	%rd<16>;
	// demoted variable
	.shared .align 2 .b8 _ZZ15sgemm_optimizedPKfS0_PfE2As[2048];
	// demoted variable
	.shared .align 2 .b8 _ZZ15sgemm_optimizedPKfS0_PfE2Bs[2048];
	// demoted variable
	.shared .align 4 .b8 _ZZ15sgemm_optimizedPKfS0_PfE6C_smem[16384];

	ld.param.u64 	%rd4, [_Z15sgemm_optimizedPKfS0_Pf_param_0];
	ld.param.u64 	%rd5, [_Z15sgemm_optimizedPKfS0_Pf_param_1];
	ld.param.u64 	%rd6, [_Z15sgemm_optimizedPKfS0_Pf_param_2];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r31, %ntid.x;
	mov.u32 	%r32, %tid.y;
	mov.u32 	%r33, %tid.x;
	mad.lo.s32 	%r113, %r32, %r31, %r33;
	shr.s32 	%r34, %r113, 31;
	shr.u32 	%r35, %r34, 25;
	add.s32 	%r36, %r113, %r35;
	shr.s32 	%r37, %r36, 7;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
	shr.u32 	%r38, %r34, 27;
	add.s32 	%r39, %r113, %r38;
	shr.s32 	%r40, %r39, 5;
	shr.s32 	%r41, %r39, 31;
	shr.u32 	%r42, %r41, 30;
	add.s32 	%r43, %r40, %r42;
	and.b32  	%r44, %r43, -4;
	sub.s32 	%r45, %r40, %r44;
	mov.u32 	%r46, %ctaid.y;
	shl.b32 	%r2, %r46, 6;
	mov.u32 	%r47, %ctaid.x;
	shl.b32 	%r3, %r47, 6;
	mov.u32 	%r48, %ntid.y;
	mul.lo.s32 	%r4, %r31, %r48;
	shl.b32 	%r5, %r37, 4;
	shl.b32 	%r6, %r45, 4;
	shl.b32 	%r49, %r37, 9;
	mov.u32 	%r50, _ZZ15sgemm_optimizedPKfS0_PfE2As;
	add.s32 	%r7, %r50, %r49;
	shl.b32 	%r51, %r45, 5;
	mov.u32 	%r52, _ZZ15sgemm_optimizedPKfS0_PfE2Bs;
	add.s32 	%r8, %r52, %r51;
	mov.u32 	%r110, 0;
	mov.f32 	%f34, 0f00000000;
	mov.f32 	%f35, %f34;
	mov.f32 	%f36, %f34;
	mov.f32 	%f37, %f34;
	mov.f32 	%f38, %f34;
	mov.f32 	%f39, %f34;
	mov.f32 	%f40, %f34;
	mov.f32 	%f41, %f34;

$L__BB0_1:
	shl.b32 	%r10, %r110, 4;
	setp.gt.s32 	%p1, %r113, 1023;
	@%p1 bra 	$L__BB0_10;

	mov.u32 	%r111, %r113;

$L__BB0_3:
	shr.s32 	%r53, %r111, 31;
	shr.u32 	%r54, %r53, 28;
	add.s32 	%r55, %r111, %r54;
	shr.s32 	%r12, %r55, 4;
	add.s32 	%r13, %r12, %r2;
	and.b32  	%r56, %r55, -16;
	sub.s32 	%r14, %r111, %r56;
	add.s32 	%r15, %r14, %r10;
	setp.gt.s32 	%p2, %r13, 1023;
	setp.gt.s32 	%p3, %r15, 1023;
	mov.f32 	%f42, 0f00000000;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB0_5;

	shl.b32 	%r57, %r13, 10;
	add.s32 	%r58, %r57, %r15;
	mul.wide.s32 	%rd7, %r58, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.f32 	%f42, [%rd8];

$L__BB0_5:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f42;}

	// end inline asm
	shl.b32 	%r59, %r12, 5;
	add.s32 	%r61, %r50, %r59;
	shl.b32 	%r62, %r14, 1;
	add.s32 	%r63, %r61, %r62;
	st.shared.u16 	[%r63], %rs1;
	add.s32 	%r111, %r111, %r4;
	setp.lt.s32 	%p5, %r111, 1024;
	@%p5 bra 	$L__BB0_3;

	mov.u32 	%r112, %r113;

$L__BB0_7:
	shr.s32 	%r64, %r112, 31;
	shr.u32 	%r65, %r64, 26;
	add.s32 	%r66, %r112, %r65;
	shr.s32 	%r18, %r66, 6;
	add.s32 	%r19, %r18, %r10;
	and.b32  	%r67, %r66, -64;
	sub.s32 	%r20, %r112, %r67;
	add.s32 	%r21, %r20, %r3;
	setp.gt.s32 	%p6, %r19, 1023;
	setp.gt.s32 	%p7, %r21, 1023;
	mov.f32 	%f43, 0f00000000;
	or.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB0_9;

	shl.b32 	%r68, %r19, 10;
	add.s32 	%r69, %r68, %r21;
	mul.wide.s32 	%rd9, %r69, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.nc.f32 	%f43, [%rd10];

$L__BB0_9:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f43;}

	// end inline asm
	shl.b32 	%r70, %r18, 7;
	add.s32 	%r72, %r52, %r70;
	shl.b32 	%r73, %r20, 1;
	add.s32 	%r74, %r72, %r73;
	st.shared.u16 	[%r74], %rs2;
	add.s32 	%r112, %r112, %r4;
	setp.lt.s32 	%p9, %r112, 1024;
	@%p9 bra 	$L__BB0_7;

$L__BB0_10:
	bar.sync 	0;
	mov.u32 	%r75, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r76, %r77, %r78, %r79, %r80, %r81, %r82, %r83}, [%r7], %r75;
	mov.u32 	%r84, 64;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r85, %r86, %r87, %r88, %r89, %r90, %r91, %r92}, [%r8], %r84;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f41, %f40, %f39, %f38, %f37, %f36, %f35, %f34}, {%r76, %r77, %r78, %r79, %r80, %r81, %r82, %r83}, {%r85, %r86, %r87, %r88, %r89, %r90, %r91, %r92}, {%f41, %f40, %f39, %f38, %f37, %f36, %f35, %f34};
	bar.sync 	0;
	add.s32 	%r110, %r110, 1;
	setp.lt.u32 	%p10, %r110, 64;
	@%p10 bra 	$L__BB0_1;

	shl.b32 	%r93, %r5, 8;
	mov.u32 	%r94, _ZZ15sgemm_optimizedPKfS0_PfE6C_smem;
	add.s32 	%r95, %r94, %r93;
	shl.b32 	%r96, %r6, 2;
	add.s32 	%r97, %r95, %r96;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f32 	[%r97], {%f41, %f40, %f39, %f38, %f37, %f36, %f35, %f34}, %r84;
	bar.sync 	0;
	setp.gt.s32 	%p11, %r113, 4095;
	@%p11 bra 	$L__BB0_15;

$L__BB0_12:
	shr.s32 	%r99, %r113, 31;
	shr.u32 	%r100, %r99, 26;
	add.s32 	%r101, %r113, %r100;
	shr.s32 	%r25, %r101, 6;
	add.s32 	%r26, %r25, %r2;
	and.b32  	%r102, %r101, -64;
	sub.s32 	%r27, %r113, %r102;
	add.s32 	%r28, %r27, %r3;
	setp.gt.s32 	%p12, %r26, 1023;
	setp.gt.s32 	%p13, %r28, 1023;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB0_14;

	shl.b32 	%r103, %r25, 8;
	add.s32 	%r105, %r94, %r103;
	shl.b32 	%r106, %r27, 2;
	add.s32 	%r107, %r105, %r106;
	ld.shared.f32 	%f33, [%r107];
	shl.b32 	%r108, %r26, 10;
	add.s32 	%r109, %r108, %r28;
	mul.wide.s32 	%rd14, %r109, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.f32 	[%rd15], %f33;

$L__BB0_14:
	add.s32 	%r113, %r113, %r4;
	setp.lt.s32 	%p15, %r113, 4096;
	@%p15 bra 	$L__BB0_12;

$L__BB0_15:
	ret;

}

