//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	_Z15sgemm_optimizedPKfS0_Pf
// _ZZ15sgemm_optimizedPKfS0_PfE2As has been demoted
// _ZZ15sgemm_optimizedPKfS0_PfE2Bs has been demoted
// _ZZ15sgemm_optimizedPKfS0_PfE6C_smem has been demoted

.visible .entry _Z15sgemm_optimizedPKfS0_Pf(
	.param .u64 _Z15sgemm_optimizedPKfS0_Pf_param_0,
	.param .u64 _Z15sgemm_optimizedPKfS0_Pf_param_1,
	.param .u64 _Z15sgemm_optimizedPKfS0_Pf_param_2
)
{
	.reg .pred 	%p<76>;
	.reg .b16 	%rs<15>;
	.reg .f32 	%f<108>;
	.reg .b32 	%r<391>;
	.reg .b64 	%rd<48>;
	// demoted variable
	.shared .align 2 .b8 _ZZ15sgemm_optimizedPKfS0_PfE2As[2048];
	// demoted variable
	.shared .align 2 .b8 _ZZ15sgemm_optimizedPKfS0_PfE2Bs[2048];
	// demoted variable
	.shared .align 4 .b8 _ZZ15sgemm_optimizedPKfS0_PfE6C_smem[16384];

	ld.param.u64 	%rd4, [_Z15sgemm_optimizedPKfS0_Pf_param_0];
	ld.param.u64 	%rd5, [_Z15sgemm_optimizedPKfS0_Pf_param_1];
	ld.param.u64 	%rd6, [_Z15sgemm_optimizedPKfS0_Pf_param_2];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r389, %tid.x;
	shr.s32 	%r115, %r389, 31;
	shr.u32 	%r116, %r115, 25;
	add.s32 	%r117, %r389, %r116;
	shr.s32 	%r118, %r117, 7;
	mov.u32 	%r119, %ctaid.x;
	shl.b32 	%r120, %r119, 2;
	and.b32  	%r2, %r120, -64;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
	shr.u32 	%r121, %r115, 27;
	add.s32 	%r122, %r389, %r121;
	shr.s32 	%r123, %r122, 5;
	shr.s32 	%r124, %r122, 31;
	shr.u32 	%r125, %r124, 30;
	add.s32 	%r126, %r123, %r125;
	and.b32  	%r127, %r126, -4;
	sub.s32 	%r128, %r123, %r127;
	shl.b32 	%r129, %r119, 6;
	and.b32  	%r3, %r129, 960;
	shl.b32 	%r130, %r118, 9;
	mov.u32 	%r131, _ZZ15sgemm_optimizedPKfS0_PfE2As;
	add.s32 	%r6, %r131, %r130;
	shl.b32 	%r132, %r128, 5;
	mov.u32 	%r133, _ZZ15sgemm_optimizedPKfS0_PfE2Bs;
	add.s32 	%r7, %r133, %r132;
	max.s32 	%r134, %r389, 512;
	add.s32 	%r135, %r134, 511;
	sub.s32 	%r8, %r135, %r389;
	shr.u32 	%r136, %r8, 9;
	add.s32 	%r137, %r136, 1;
	and.b32  	%r9, %r137, 3;
	shr.u32 	%r138, %r115, 28;
	add.s32 	%r139, %r389, %r138;
	shr.s32 	%r140, %r139, 4;
	and.b32  	%r141, %r139, -16;
	sub.s32 	%r10, %r389, %r141;
	add.s32 	%r11, %r140, %r2;
	shl.b32 	%r12, %r11, 10;
	shl.b32 	%r142, %r140, 5;
	add.s32 	%r143, %r131, %r142;
	shl.b32 	%r144, %r10, 1;
	add.s32 	%r13, %r143, %r144;
	add.s32 	%r14, %r389, 512;
	shr.u32 	%r145, %r115, 26;
	add.s32 	%r146, %r389, %r145;
	shr.s32 	%r15, %r146, 6;
	and.b32  	%r147, %r146, -64;
	sub.s32 	%r148, %r389, %r147;
	add.s32 	%r16, %r148, %r3;
	shr.s32 	%r149, %r14, 31;
	shr.u32 	%r150, %r149, 28;
	add.s32 	%r151, %r14, %r150;
	shr.s32 	%r152, %r151, 4;
	and.b32  	%r153, %r151, -16;
	sub.s32 	%r17, %r14, %r153;
	add.s32 	%r18, %r152, %r2;
	shl.b32 	%r154, %r15, 7;
	add.s32 	%r155, %r133, %r154;
	shl.b32 	%r156, %r148, 1;
	add.s32 	%r19, %r155, %r156;
	shl.b32 	%r20, %r18, 10;
	shl.b32 	%r157, %r152, 5;
	add.s32 	%r158, %r131, %r157;
	shl.b32 	%r159, %r17, 1;
	add.s32 	%r21, %r158, %r159;
	add.s32 	%r22, %r389, 1024;
	shr.u32 	%r160, %r149, 26;
	add.s32 	%r161, %r14, %r160;
	shr.s32 	%r23, %r161, 6;
	and.b32  	%r162, %r161, -64;
	sub.s32 	%r163, %r14, %r162;
	add.s32 	%r24, %r163, %r3;
	shr.s32 	%r164, %r22, 31;
	shr.u32 	%r165, %r164, 28;
	add.s32 	%r166, %r22, %r165;
	shr.s32 	%r167, %r166, 4;
	and.b32  	%r168, %r166, -16;
	sub.s32 	%r25, %r22, %r168;
	add.s32 	%r26, %r167, %r2;
	shl.b32 	%r169, %r23, 7;
	add.s32 	%r170, %r133, %r169;
	shl.b32 	%r171, %r163, 1;
	add.s32 	%r27, %r170, %r171;
	shl.b32 	%r28, %r26, 10;
	shl.b32 	%r172, %r167, 5;
	add.s32 	%r173, %r131, %r172;
	shl.b32 	%r174, %r25, 1;
	add.s32 	%r29, %r173, %r174;
	add.s32 	%r30, %r389, 1536;
	shr.u32 	%r175, %r164, 26;
	add.s32 	%r176, %r22, %r175;
	shr.s32 	%r31, %r176, 6;
	and.b32  	%r177, %r176, -64;
	sub.s32 	%r178, %r22, %r177;
	add.s32 	%r32, %r178, %r3;
	shl.b32 	%r179, %r31, 7;
	add.s32 	%r180, %r133, %r179;
	shl.b32 	%r181, %r178, 1;
	add.s32 	%r33, %r180, %r181;
	mov.u32 	%r382, 0;
	mov.f32 	%f86, 0f00000000;
	mov.f32 	%f87, %f86;
	mov.f32 	%f88, %f86;
	mov.f32 	%f89, %f86;
	mov.f32 	%f90, %f86;
	mov.f32 	%f91, %f86;
	mov.f32 	%f92, %f86;
	mov.f32 	%f93, %f86;

$L__BB0_1:
	shl.b32 	%r35, %r382, 4;
	setp.gt.s32 	%p1, %r389, 1023;
	@%p1 bra 	$L__BB0_22;

	setp.eq.s32 	%p2, %r9, 0;
	mov.u32 	%r383, %r389;
	@%p2 bra 	$L__BB0_12;

	setp.gt.s32 	%p3, %r11, 1023;
	add.s32 	%r36, %r10, %r35;
	setp.gt.s32 	%p4, %r36, 1023;
	mov.f32 	%f94, 0f00000000;
	or.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_5;

	add.s32 	%r182, %r12, %r36;
	mul.wide.s32 	%rd7, %r182, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.f32 	%f94, [%rd8];

$L__BB0_5:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f94;}

	// end inline asm
	st.shared.u16 	[%r13], %rs1;
	setp.eq.s32 	%p6, %r9, 1;
	mov.u32 	%r383, %r14;
	@%p6 bra 	$L__BB0_12;

	setp.gt.s32 	%p7, %r18, 1023;
	add.s32 	%r37, %r17, %r35;
	setp.gt.s32 	%p8, %r37, 1023;
	mov.f32 	%f95, 0f00000000;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB0_8;

	add.s32 	%r183, %r20, %r37;
	mul.wide.s32 	%rd9, %r183, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f95, [%rd10];

$L__BB0_8:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f95;}

	// end inline asm
	st.shared.u16 	[%r21], %rs2;
	setp.eq.s32 	%p10, %r9, 2;
	mov.u32 	%r383, %r22;
	@%p10 bra 	$L__BB0_12;

	setp.gt.s32 	%p11, %r26, 1023;
	add.s32 	%r38, %r25, %r35;
	setp.gt.s32 	%p12, %r38, 1023;
	mov.f32 	%f96, 0f00000000;
	or.pred  	%p13, %p11, %p12;
	@%p13 bra 	$L__BB0_11;

	add.s32 	%r184, %r28, %r38;
	mul.wide.s32 	%rd11, %r184, 4;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.nc.f32 	%f96, [%rd12];

$L__BB0_11:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f96;}

	// end inline asm
	st.shared.u16 	[%r29], %rs3;
	mov.u32 	%r383, %r30;

$L__BB0_12:
	setp.lt.u32 	%p14, %r8, 1536;
	@%p14 bra 	$L__BB0_22;

$L__BB0_13:
	shr.s32 	%r185, %r383, 31;
	shr.u32 	%r186, %r185, 28;
	add.s32 	%r187, %r383, %r186;
	shr.s32 	%r41, %r187, 4;
	add.s32 	%r42, %r41, %r2;
	and.b32  	%r188, %r187, -16;
	sub.s32 	%r43, %r383, %r188;
	add.s32 	%r44, %r43, %r35;
	setp.gt.s32 	%p15, %r42, 1023;
	setp.gt.s32 	%p16, %r44, 1023;
	mov.f32 	%f98, 0f00000000;
	or.pred  	%p17, %p15, %p16;
	mov.f32 	%f97, %f98;
	@%p17 bra 	$L__BB0_15;

	shl.b32 	%r189, %r42, 10;
	add.s32 	%r190, %r189, %r44;
	mul.wide.s32 	%rd13, %r190, 4;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.nc.f32 	%f97, [%rd14];

$L__BB0_15:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f97;}

	// end inline asm
	shl.b32 	%r191, %r41, 5;
	add.s32 	%r193, %r131, %r191;
	shl.b32 	%r194, %r43, 1;
	add.s32 	%r195, %r193, %r194;
	st.shared.u16 	[%r195], %rs4;
	add.s32 	%r45, %r383, 512;
	shr.s32 	%r196, %r45, 31;
	shr.u32 	%r197, %r196, 28;
	add.s32 	%r198, %r45, %r197;
	shr.s32 	%r46, %r198, 4;
	and.b32  	%r199, %r198, -16;
	sub.s32 	%r47, %r45, %r199;
	add.s32 	%r48, %r46, %r2;
	add.s32 	%r49, %r47, %r35;
	setp.gt.s32 	%p18, %r48, 1023;
	setp.gt.s32 	%p19, %r49, 1023;
	or.pred  	%p20, %p18, %p19;
	@%p20 bra 	$L__BB0_17;

	shl.b32 	%r200, %r48, 10;
	add.s32 	%r201, %r200, %r49;
	mul.wide.s32 	%rd15, %r201, 4;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.nc.f32 	%f98, [%rd16];

$L__BB0_17:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f98;}

	// end inline asm
	shl.b32 	%r202, %r46, 5;
	add.s32 	%r204, %r131, %r202;
	shl.b32 	%r205, %r47, 1;
	add.s32 	%r206, %r204, %r205;
	st.shared.u16 	[%r206], %rs5;
	add.s32 	%r50, %r45, 512;
	shr.s32 	%r207, %r50, 31;
	shr.u32 	%r208, %r207, 28;
	add.s32 	%r209, %r50, %r208;
	shr.s32 	%r51, %r209, 4;
	and.b32  	%r210, %r209, -16;
	sub.s32 	%r52, %r50, %r210;
	add.s32 	%r53, %r51, %r2;
	add.s32 	%r54, %r52, %r35;
	setp.gt.s32 	%p21, %r53, 1023;
	setp.gt.s32 	%p22, %r54, 1023;
	mov.f32 	%f100, 0f00000000;
	or.pred  	%p23, %p21, %p22;
	mov.f32 	%f99, %f100;
	@%p23 bra 	$L__BB0_19;

	shl.b32 	%r211, %r53, 10;
	add.s32 	%r212, %r211, %r54;
	mul.wide.s32 	%rd17, %r212, 4;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.f32 	%f99, [%rd18];

$L__BB0_19:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f99;}

	// end inline asm
	shl.b32 	%r213, %r51, 5;
	add.s32 	%r215, %r131, %r213;
	shl.b32 	%r216, %r52, 1;
	add.s32 	%r217, %r215, %r216;
	st.shared.u16 	[%r217], %rs6;
	add.s32 	%r218, %r50, 512;
	shr.s32 	%r219, %r218, 31;
	shr.u32 	%r220, %r219, 28;
	add.s32 	%r221, %r218, %r220;
	shr.s32 	%r55, %r221, 4;
	and.b32  	%r222, %r221, -16;
	sub.s32 	%r56, %r218, %r222;
	add.s32 	%r57, %r55, %r2;
	add.s32 	%r58, %r56, %r35;
	setp.gt.s32 	%p24, %r57, 1023;
	setp.gt.s32 	%p25, %r58, 1023;
	or.pred  	%p26, %p24, %p25;
	@%p26 bra 	$L__BB0_21;

	shl.b32 	%r223, %r57, 10;
	add.s32 	%r224, %r223, %r58;
	mul.wide.s32 	%rd19, %r224, 4;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.nc.f32 	%f100, [%rd20];

$L__BB0_21:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f100;}

	// end inline asm
	shl.b32 	%r225, %r55, 5;
	add.s32 	%r227, %r131, %r225;
	shl.b32 	%r228, %r56, 1;
	add.s32 	%r229, %r227, %r228;
	st.shared.u16 	[%r229], %rs7;
	add.s32 	%r59, %r383, 2048;
	setp.lt.s32 	%p27, %r383, -1024;
	mov.u32 	%r383, %r59;
	@%p27 bra 	$L__BB0_13;

$L__BB0_22:
	@%p1 bra 	$L__BB0_43;

	setp.eq.s32 	%p29, %r9, 0;
	mov.u32 	%r385, %r389;
	@%p29 bra 	$L__BB0_33;

	setp.gt.s32 	%p30, %r16, 1023;
	add.s32 	%r60, %r15, %r35;
	setp.gt.s32 	%p31, %r60, 1023;
	mov.f32 	%f101, 0f00000000;
	or.pred  	%p32, %p31, %p30;
	@%p32 bra 	$L__BB0_26;

	shl.b32 	%r230, %r60, 10;
	add.s32 	%r231, %r230, %r16;
	mul.wide.s32 	%rd21, %r231, 4;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.f32 	%f101, [%rd22];

$L__BB0_26:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f101;}

	// end inline asm
	st.shared.u16 	[%r19], %rs8;
	setp.eq.s32 	%p33, %r9, 1;
	mov.u32 	%r385, %r14;
	@%p33 bra 	$L__BB0_33;

	setp.gt.s32 	%p34, %r24, 1023;
	add.s32 	%r61, %r23, %r35;
	setp.gt.s32 	%p35, %r61, 1023;
	mov.f32 	%f102, 0f00000000;
	or.pred  	%p36, %p35, %p34;
	@%p36 bra 	$L__BB0_29;

	shl.b32 	%r232, %r61, 10;
	add.s32 	%r233, %r232, %r24;
	mul.wide.s32 	%rd23, %r233, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.f32 	%f102, [%rd24];

$L__BB0_29:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f102;}

	// end inline asm
	st.shared.u16 	[%r27], %rs9;
	setp.eq.s32 	%p37, %r9, 2;
	mov.u32 	%r385, %r22;
	@%p37 bra 	$L__BB0_33;

	setp.gt.s32 	%p38, %r32, 1023;
	add.s32 	%r62, %r31, %r35;
	setp.gt.s32 	%p39, %r62, 1023;
	mov.f32 	%f103, 0f00000000;
	or.pred  	%p40, %p39, %p38;
	@%p40 bra 	$L__BB0_32;

	shl.b32 	%r234, %r62, 10;
	add.s32 	%r235, %r234, %r32;
	mul.wide.s32 	%rd25, %r235, 4;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.nc.f32 	%f103, [%rd26];

$L__BB0_32:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f103;}

	// end inline asm
	st.shared.u16 	[%r33], %rs10;
	mov.u32 	%r385, %r30;

$L__BB0_33:
	setp.lt.u32 	%p41, %r8, 1536;
	@%p41 bra 	$L__BB0_43;

$L__BB0_34:
	shr.s32 	%r236, %r385, 31;
	shr.u32 	%r237, %r236, 26;
	add.s32 	%r238, %r385, %r237;
	shr.s32 	%r65, %r238, 6;
	add.s32 	%r66, %r65, %r35;
	and.b32  	%r239, %r238, -64;
	sub.s32 	%r67, %r385, %r239;
	add.s32 	%r68, %r67, %r3;
	setp.gt.s32 	%p42, %r66, 1023;
	setp.gt.s32 	%p43, %r68, 1023;
	mov.f32 	%f105, 0f00000000;
	or.pred  	%p44, %p42, %p43;
	mov.f32 	%f104, %f105;
	@%p44 bra 	$L__BB0_36;

	shl.b32 	%r240, %r66, 10;
	add.s32 	%r241, %r240, %r68;
	mul.wide.s32 	%rd27, %r241, 4;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.nc.f32 	%f104, [%rd28];

$L__BB0_36:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f104;}

	// end inline asm
	shl.b32 	%r242, %r65, 7;
	add.s32 	%r244, %r133, %r242;
	shl.b32 	%r245, %r67, 1;
	add.s32 	%r246, %r244, %r245;
	st.shared.u16 	[%r246], %rs11;
	add.s32 	%r247, %r385, 512;
	shr.s32 	%r248, %r247, 31;
	shr.u32 	%r249, %r248, 26;
	add.s32 	%r250, %r247, %r249;
	shr.s32 	%r69, %r250, 6;
	and.b32  	%r251, %r250, -64;
	sub.s32 	%r70, %r247, %r251;
	add.s32 	%r71, %r69, %r35;
	add.s32 	%r72, %r70, %r3;
	setp.gt.s32 	%p45, %r71, 1023;
	setp.gt.s32 	%p46, %r72, 1023;
	or.pred  	%p47, %p45, %p46;
	@%p47 bra 	$L__BB0_38;

	shl.b32 	%r252, %r71, 10;
	add.s32 	%r253, %r252, %r72;
	mul.wide.s32 	%rd29, %r253, 4;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.nc.f32 	%f105, [%rd30];

$L__BB0_38:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f105;}

	// end inline asm
	shl.b32 	%r254, %r69, 7;
	add.s32 	%r256, %r133, %r254;
	shl.b32 	%r257, %r70, 1;
	add.s32 	%r258, %r256, %r257;
	st.shared.u16 	[%r258], %rs12;
	add.s32 	%r259, %r385, 1024;
	shr.s32 	%r260, %r259, 31;
	shr.u32 	%r261, %r260, 26;
	add.s32 	%r262, %r259, %r261;
	shr.s32 	%r73, %r262, 6;
	and.b32  	%r263, %r262, -64;
	sub.s32 	%r74, %r259, %r263;
	add.s32 	%r75, %r73, %r35;
	add.s32 	%r76, %r74, %r3;
	setp.gt.s32 	%p48, %r75, 1023;
	setp.gt.s32 	%p49, %r76, 1023;
	mov.f32 	%f107, 0f00000000;
	or.pred  	%p50, %p48, %p49;
	mov.f32 	%f106, %f107;
	@%p50 bra 	$L__BB0_40;

	shl.b32 	%r264, %r75, 10;
	add.s32 	%r265, %r264, %r76;
	mul.wide.s32 	%rd31, %r265, 4;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.nc.f32 	%f106, [%rd32];

$L__BB0_40:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f106;}

	// end inline asm
	shl.b32 	%r266, %r73, 7;
	add.s32 	%r268, %r133, %r266;
	shl.b32 	%r269, %r74, 1;
	add.s32 	%r270, %r268, %r269;
	st.shared.u16 	[%r270], %rs13;
	add.s32 	%r271, %r385, 1536;
	shr.s32 	%r272, %r271, 31;
	shr.u32 	%r273, %r272, 26;
	add.s32 	%r274, %r271, %r273;
	shr.s32 	%r77, %r274, 6;
	and.b32  	%r275, %r274, -64;
	sub.s32 	%r78, %r271, %r275;
	add.s32 	%r79, %r77, %r35;
	add.s32 	%r80, %r78, %r3;
	setp.gt.s32 	%p51, %r79, 1023;
	setp.gt.s32 	%p52, %r80, 1023;
	or.pred  	%p53, %p51, %p52;
	@%p53 bra 	$L__BB0_42;

	shl.b32 	%r276, %r79, 10;
	add.s32 	%r277, %r276, %r80;
	mul.wide.s32 	%rd33, %r277, 4;
	add.s64 	%rd34, %rd1, %rd33;
	ld.global.nc.f32 	%f107, [%rd34];

$L__BB0_42:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f107;}

	// end inline asm
	shl.b32 	%r278, %r77, 7;
	add.s32 	%r280, %r133, %r278;
	shl.b32 	%r281, %r78, 1;
	add.s32 	%r282, %r280, %r281;
	st.shared.u16 	[%r282], %rs14;
	add.s32 	%r81, %r385, 2048;
	setp.lt.s32 	%p54, %r385, -1024;
	mov.u32 	%r385, %r81;
	@%p54 bra 	$L__BB0_34;

$L__BB0_43:
	bar.sync 	0;
	mov.u32 	%r283, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r284, %r285, %r286, %r287, %r288, %r289, %r290, %r291}, [%r6], %r283;
	mov.u32 	%r292, 64;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r293, %r294, %r295, %r296, %r297, %r298, %r299, %r300}, [%r7], %r292;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f93, %f92, %f91, %f90, %f89, %f88, %f87, %f86}, {%r284, %r285, %r286, %r287, %r288, %r289, %r290, %r291}, {%r293, %r294, %r295, %r296, %r297, %r298, %r299, %r300}, {%f93, %f92, %f91, %f90, %f89, %f88, %f87, %f86};
	bar.sync 	0;
	add.s32 	%r382, %r382, 1;
	setp.lt.u32 	%p55, %r382, 64;
	@%p55 bra 	$L__BB0_1;

	shr.s32 	%r381, %r389, 31;
	shr.u32 	%r380, %r381, 27;
	add.s32 	%r379, %r389, %r380;
	shr.s32 	%r378, %r379, 5;
	mov.u32 	%r377, 64;
	shr.s32 	%r376, %r379, 31;
	shr.u32 	%r375, %r376, 30;
	add.s32 	%r374, %r378, %r375;
	and.b32  	%r373, %r374, -4;
	sub.s32 	%r372, %r378, %r373;
	shl.b32 	%r371, %r372, 4;
	shr.u32 	%r370, %r381, 25;
	add.s32 	%r369, %r389, %r370;
	shr.s32 	%r368, %r369, 7;
	shl.b32 	%r367, %r368, 4;
	shl.b32 	%r301, %r367, 8;
	mov.u32 	%r302, _ZZ15sgemm_optimizedPKfS0_PfE6C_smem;
	add.s32 	%r303, %r302, %r301;
	shl.b32 	%r304, %r371, 2;
	add.s32 	%r305, %r303, %r304;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f32 	[%r305], {%f93, %f92, %f91, %f90, %f89, %f88, %f87, %f86}, %r377;
	bar.sync 	0;
	setp.gt.s32 	%p56, %r389, 4095;
	@%p56 bra 	$L__BB0_59;

	max.s32 	%r307, %r389, 3584;
	add.s32 	%r308, %r307, 511;
	sub.s32 	%r83, %r308, %r389;
	shr.u32 	%r309, %r83, 9;
	add.s32 	%r310, %r309, 1;
	and.b32  	%r388, %r310, 3;
	setp.eq.s32 	%p57, %r388, 0;
	@%p57 bra 	$L__BB0_49;

$L__BB0_46:
	.pragma "nounroll";
	shr.s32 	%r311, %r389, 31;
	shr.u32 	%r312, %r311, 26;
	add.s32 	%r313, %r389, %r312;
	shr.s32 	%r87, %r313, 6;
	add.s32 	%r88, %r87, %r2;
	and.b32  	%r314, %r313, -64;
	sub.s32 	%r89, %r389, %r314;
	add.s32 	%r90, %r89, %r3;
	setp.gt.s32 	%p58, %r88, 1023;
	setp.gt.s32 	%p59, %r90, 1023;
	or.pred  	%p60, %p58, %p59;
	@%p60 bra 	$L__BB0_48;

	shl.b32 	%r315, %r87, 8;
	add.s32 	%r317, %r302, %r315;
	shl.b32 	%r318, %r89, 2;
	add.s32 	%r319, %r317, %r318;
	ld.shared.f32 	%f81, [%r319];
	shl.b32 	%r320, %r88, 10;
	add.s32 	%r321, %r320, %r90;
	mul.wide.s32 	%rd38, %r321, 4;
	add.s64 	%rd39, %rd3, %rd38;
	st.global.f32 	[%rd39], %f81;

$L__BB0_48:
	add.s32 	%r389, %r389, 512;
	add.s32 	%r388, %r388, -1;
	setp.ne.s32 	%p61, %r388, 0;
	@%p61 bra 	$L__BB0_46;

$L__BB0_49:
	setp.lt.u32 	%p62, %r83, 1536;
	@%p62 bra 	$L__BB0_59;

$L__BB0_50:
	shr.s32 	%r322, %r389, 31;
	shr.u32 	%r323, %r322, 26;
	add.s32 	%r324, %r389, %r323;
	shr.s32 	%r95, %r324, 6;
	add.s32 	%r96, %r95, %r2;
	and.b32  	%r325, %r324, -64;
	sub.s32 	%r97, %r389, %r325;
	add.s32 	%r98, %r97, %r3;
	setp.gt.s32 	%p63, %r96, 1023;
	setp.gt.s32 	%p64, %r98, 1023;
	or.pred  	%p65, %p63, %p64;
	@%p65 bra 	$L__BB0_52;

	shl.b32 	%r326, %r95, 8;
	add.s32 	%r328, %r302, %r326;
	shl.b32 	%r329, %r97, 2;
	add.s32 	%r330, %r328, %r329;
	ld.shared.f32 	%f82, [%r330];
	shl.b32 	%r331, %r96, 10;
	add.s32 	%r332, %r331, %r98;
	mul.wide.s32 	%rd40, %r332, 4;
	add.s64 	%rd41, %rd3, %rd40;
	st.global.f32 	[%rd41], %f82;

$L__BB0_52:
	add.s32 	%r99, %r389, 512;
	shr.s32 	%r333, %r99, 31;
	shr.u32 	%r334, %r333, 26;
	add.s32 	%r335, %r99, %r334;
	shr.s32 	%r100, %r335, 6;
	and.b32  	%r336, %r335, -64;
	sub.s32 	%r101, %r99, %r336;
	add.s32 	%r102, %r100, %r2;
	add.s32 	%r103, %r101, %r3;
	setp.gt.s32 	%p66, %r102, 1023;
	setp.gt.s32 	%p67, %r103, 1023;
	or.pred  	%p68, %p66, %p67;
	@%p68 bra 	$L__BB0_54;

	shl.b32 	%r337, %r100, 8;
	add.s32 	%r339, %r302, %r337;
	shl.b32 	%r340, %r101, 2;
	add.s32 	%r341, %r339, %r340;
	ld.shared.f32 	%f83, [%r341];
	shl.b32 	%r342, %r102, 10;
	add.s32 	%r343, %r342, %r103;
	mul.wide.s32 	%rd42, %r343, 4;
	add.s64 	%rd43, %rd3, %rd42;
	st.global.f32 	[%rd43], %f83;

$L__BB0_54:
	add.s32 	%r104, %r99, 512;
	shr.s32 	%r344, %r104, 31;
	shr.u32 	%r345, %r344, 26;
	add.s32 	%r346, %r104, %r345;
	shr.s32 	%r105, %r346, 6;
	and.b32  	%r347, %r346, -64;
	sub.s32 	%r106, %r104, %r347;
	add.s32 	%r107, %r105, %r2;
	add.s32 	%r108, %r106, %r3;
	setp.gt.s32 	%p69, %r107, 1023;
	setp.gt.s32 	%p70, %r108, 1023;
	or.pred  	%p71, %p69, %p70;
	@%p71 bra 	$L__BB0_56;

	shl.b32 	%r348, %r105, 8;
	add.s32 	%r350, %r302, %r348;
	shl.b32 	%r351, %r106, 2;
	add.s32 	%r352, %r350, %r351;
	ld.shared.f32 	%f84, [%r352];
	shl.b32 	%r353, %r107, 10;
	add.s32 	%r354, %r353, %r108;
	mul.wide.s32 	%rd44, %r354, 4;
	add.s64 	%rd45, %rd3, %rd44;
	st.global.f32 	[%rd45], %f84;

$L__BB0_56:
	add.s32 	%r355, %r104, 512;
	shr.s32 	%r356, %r355, 31;
	shr.u32 	%r357, %r356, 26;
	add.s32 	%r358, %r355, %r357;
	shr.s32 	%r109, %r358, 6;
	and.b32  	%r359, %r358, -64;
	sub.s32 	%r110, %r355, %r359;
	add.s32 	%r111, %r109, %r2;
	add.s32 	%r112, %r110, %r3;
	setp.gt.s32 	%p72, %r111, 1023;
	setp.gt.s32 	%p73, %r112, 1023;
	or.pred  	%p74, %p72, %p73;
	@%p74 bra 	$L__BB0_58;

	shl.b32 	%r360, %r109, 8;
	add.s32 	%r362, %r302, %r360;
	shl.b32 	%r363, %r110, 2;
	add.s32 	%r364, %r362, %r363;
	ld.shared.f32 	%f85, [%r364];
	shl.b32 	%r365, %r111, 10;
	add.s32 	%r366, %r365, %r112;
	mul.wide.s32 	%rd46, %r366, 4;
	add.s64 	%rd47, %rd3, %rd46;
	st.global.f32 	[%rd47], %f85;

$L__BB0_58:
	setp.lt.s32 	%p75, %r389, 2048;
	add.s32 	%r389, %r389, 2048;
	@%p75 bra 	$L__BB0_50;

$L__BB0_59:
	ret;

}

